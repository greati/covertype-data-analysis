{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative hierarchical clustering\n",
    "\n",
    "In this technique, each data point is a cluster in the begginning. Then, join the two closest data points,\n",
    "based on some distance, into one cluster. Run this process until one big cluster is formed. Then, \n",
    "divide the resulting hierarchical clustering into the desired number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Number of groups\n",
    "\n",
    "Agglomerative clustering allows to define the number of groups at the end of the execution. This work will\n",
    "consider $N \\in \\{2,3,\\ldots,13\\}$ groups. Since the algorithm is deterministic, only one executing per $N$\n",
    "value will be performed.\n",
    "\n",
    "### Davies-Bouldin (DB) index\n",
    "\n",
    "Every clustering will then be evaluated in the view of the Davies-Bouldin (DB) index:\n",
    "\n",
    "$$\n",
    "R_{ij} = \\frac{s_i + s_j}{d_{ij}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "D_i = \\max_{j\\neq i}R_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "DB = \\frac 1 k \\sum_i D_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(paths):\n",
    "    '''\n",
    "    Load the datasets in paths.\n",
    "    '''\n",
    "    # load train and test datasets together\n",
    "    dataset = pd.concat([pd.read_csv(f) for f in paths])\n",
    "    # keep targets for future comparative purposes\n",
    "    targets = dataset['cover_type']\n",
    "    # remove target column\n",
    "    dataset.drop('cover_type', inplace=True, axis=1)\n",
    "    # check shape\n",
    "    print(\"[INFO] Dataset shape: \", dataset.shape)\n",
    "    # return\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "def perform_agg_clustering(dataset, cluster_sizes):\n",
    "    '''\n",
    "    Perform aggregation clustering.\n",
    "    '''\n",
    "    results = []\n",
    "    models = {}\n",
    "    for size in cluster_sizes:\n",
    "        print(\"[INFO] Running for size\", size)\n",
    "        # train model\n",
    "        cluster = AgglomerativeClustering(n_clusters=size, affinity='euclidean', linkage='ward')\n",
    "        cluster.fit(dataset)\n",
    "        # compute db\n",
    "        db = davies_bouldin_score(dataset, cluster.labels_)\n",
    "        # store result\n",
    "        results.append([size, db])\n",
    "        # store models\n",
    "        models[size] = cluster\n",
    "        print(\"[INFO] Done for size\", size, \"db =\",db)\n",
    "    return (models, pd.DataFrame(results, columns=['cluster_size','db_index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to save models and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "def save_models_results(results, models, save_model_path, save_results_path):\n",
    "    '''\n",
    "    Save clustering models and results.\n",
    "    '''\n",
    "    for size in models:\n",
    "        joblib.dump(models[size], save_model_path + '_' + str(size))\n",
    "    results.to_csv(save_results_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to perform the entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_and_save_agg(paths, cluster_sizes, save_model_path, save_results_path):\n",
    "    '''\n",
    "    Run clustering and save models and results.\n",
    "    '''\n",
    "    dataset = load_dataset(paths)\n",
    "    models, df_results = perform_agg_clustering(dataset, cluster_sizes)\n",
    "    save_models_results(df_results, models, save_model_path, save_results_path)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute for the given datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = list(range(2,14))\n",
    "# original dataset\n",
    "original_res = perform_and_save_agg(['../datasets/covertype_norm_train.csv', \n",
    "                      '../datasets/covertype_norm_test.csv'],\n",
    "                    sizes, \n",
    "                    '../models/agg_clustering_original.save',\n",
    "                    '../results/agg_clustering_original.csv')\n",
    "# lda dataset\n",
    "lda_res = perform_and_save_agg(['../datasets/covertype_lda_train.csv', \n",
    "                       '../datasets/covertype_lda_test.csv'],\n",
    "                    sizes, \n",
    "                    '../models/agg_clustering_lda.save',\n",
    "                    '../results/agg_clustering_lda.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Results are expressed in terms of the DB index:\n",
    "\n",
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_res.plot.line(x='cluster_size', y='db_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_res.plot.line(x='cluster_size', y='db_index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
