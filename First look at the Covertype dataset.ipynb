{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowing the Covertype dataset\n",
    "\n",
    "## Description\n",
    "\n",
    "The Covertype dataset aims to provide cartographic variables (no remotely sensed data) for predicting forest cover types. Data is about four wilderness areas located in the Roosevelt National Forest of northern Colorado, whose cover types are more a result of ecological processes rather than human-caused disturbances.\n",
    "\n",
    "The original dataset contains 12 attributes (10 quantitative, 2 qualitative), but the collected data is organized in 54 columns, where 10 correspond to the quantitative variables, 4 are binary for encoding wilderness areas and 40 are also binary for encoding the soil type. A more detailed description is given in the table below:\n",
    "\n",
    "| Attribute name           | Type             | Measurement    | Description                          |\n",
    "| ------------------------ |:-----------------|:---------------|-------------------------------------:|\n",
    "| elevation                | quantitative     | meters         | Elevation in meters                  |\n",
    "| aspect                   | quantitative     | azimuth        | Aspect in degress azimuth            |\n",
    "| slope                    | quantitative     | degrees        | Slope in degress                     |\n",
    "| horiz_dist_hydro         | quantitative     | meters         | Horiz. Dist. to nearest surface water|    \n",
    "| vert_dist_hydro          | quantitative     | meters         | Vert. Dist. to nearest surface water |    \n",
    "| horiz_dist_road          | quantitative     | meters         | Horiz. Dist. to nearest roadway      |    \n",
    "| hillshade_9              | quantitative     | 0 to 255 index | Hillshade index at 9am, summer solstice|\n",
    "| hillshade_noon              | quantitative     | 0 to 255 index | Hillshade index at noon, summer solstice|\n",
    "| hillshade_15              | quantitative     | 0 to 255 index | Hillshade index at 3pm, summer solstice|\n",
    "| horiz_dist_fire          | quantitative     | meters         | Horiz. Dist. to nearest wildfire ignition points |    \n",
    "| wild_area[0-4]           | qualitative (4 classes)     | binary         | Wilderness area designation          |\n",
    "| soil_type[0-39]          | qualitative (40 classes)     | binary         | Soil type designation                |\n",
    "| cover_type               | integer          | 1 to 7         | Forest cover type designation        |\n",
    "\n",
    "The classification problem consists in classifying the forest cover into seven types:\n",
    "\n",
    "| Number | Type |           |\n",
    "| -------|------|-----------|\n",
    "| 1      | Spruce/Fir|<img src=\"imgs/spruce.jpg\" width=\"150px\" height=\"150px\"/>|\n",
    "| 2      | Lodgepole Pine|<img src=\"imgs/lodge.jpg\" width=\"150px\" height=\"150px\"/>|\n",
    "| 3      | Ponderosa Pine|<img src=\"imgs/ponderosa.jpg\" width=\"150px\" height=\"150px\"/>|\n",
    "| 4      | Cottonwood/Willow|<img src=\"imgs/cottonwood.jpg\" width=\"150px\" height=\"150px\"/>|\n",
    "| 5      | Aspen|<img src=\"imgs/aspen.jpg\" width=\"150px\" height=\"150px\"/>|\n",
    "| 6      | Douglas-fir|<img src=\"imgs/douglas.jpg\" width=\"150px\" height=\"150px\"/>|\n",
    "| 7      | Krummholz|<img src=\"imgs/krumm.jpg\" width=\"150px\" height=\"150px\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and improving columns description\n",
    "\n",
    "Below is the code for loading and previewing the raw dataset using the `pandas` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read data as csv\n",
    "dataset = pd.read_csv(\"datasets/covtype.data\", header=None)\n",
    "# preview the five first lines\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the header doesn't exist in the original dataset, so the column indexing must be done only with integers. It is possible to improve this by changing the column indices to a more readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "column_names = [\"elevation\", \"aspect\", \"slope\", \\\n",
    "                \"horiz_dist_hydro\", \"vert_dist_hydro\", \\\n",
    "                \"horiz_dist_road\", \"hillshade_9\", \\\n",
    "                \"hill_shade_noon\", \"hill_shade_15\", \"horiz_dist_fire\"] \\\n",
    "                + [\"wild_area_\" + str(i) for i in range(0,4)] \\\n",
    "                + [\"soil_type_\" + str(i) for i in range(0,40)] \\\n",
    "                + [\"cover_type\"]\n",
    "# change column names in dataframe\n",
    "dataset.columns = column_names\n",
    "# confirm the dataset size\n",
    "print(\"Dataset shape: \" + str(dataset.shape))\n",
    "# check the resulting dataset with column names\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with class imbalancing\n",
    "\n",
    "This is the distribution of rows per classes:\n",
    "\n",
    "| Type | Number of rows |\n",
    "| -----|----------------|\n",
    "| Spruce/Fir | 211840|\n",
    "| Lodgepole Pine| 283301|\n",
    "| Ponderosa Pine | 35754|\n",
    "| Corronwood/Willow| 2747|\n",
    "| Aspen | 9493|\n",
    "| Douglas-fir | 17367|\n",
    "| Krummholz | 20510|\n",
    "| **Total** | **581012**|\n",
    "\n",
    "One can notice that this is a very unbalanced dataset, since there is a huge difference between the amount of individuals between classes. Since this dataset has a lot of instances and the minimum of the number of instances in a class seems still to be substantial, the dataset will be reduced in a way that every class will have 2747 instances. In this work, random 2747 instances in each class will be selected, and the rest will be kept in another dataset. The code below does the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# group by cover type\n",
    "groups = dataset.groupby(\"cover_type\")\n",
    "# get minimum number of instances in a class\n",
    "number_samples = groups.size().min()\n",
    "# produce the new dataset\n",
    "new_dataset = pd.concat([resample(df, replace=True, \\\n",
    "                                  n_samples=number_samples, \\\n",
    "                                  random_state=123) for _, df in groups])\n",
    "# keeps the remaining dataset\n",
    "remaining_dataset = pd.concat([dataset, new_dataset]).drop_duplicates(keep=False)\n",
    "# check sizes\n",
    "print(\"New dataset shape:\" + str(new_dataset.shape))\n",
    "print(\"Remaining dataset shape:\" + str(remaining_dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values analysis\n",
    "\n",
    "There are no missing values in this dataset. In order to check this fact, it is possible to use the `missingno` library to generate a simple visualization of the new dataset (the dataset of remaining instances would behave the same), where the color white means missing values in a column (notice how everything is dark for this dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "# improve plotting visualization\n",
    "%matplotlib inline\n",
    "# plot a graph showing the missing values (in this case, there are none)\n",
    "msno.matrix(new_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the new datasets\n",
    "\n",
    "After generating the new dataset and the dataset with the remaining values, their csv versions need to be stored, in order to use them in the next steps of this work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save both datasets\n",
    "new_dataset.to_csv(\"datasets/new_dataset_covertype.csv\", index=False)\n",
    "remaining_dataset.to_csv(\"datasets/remaining_dataset_covertype.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
